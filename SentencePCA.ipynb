{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy import displacy\n",
    "import scipy.stats\n",
    "\n",
    "from spacy.tokens import Doc\n",
    "# Tokenize only on whitespace to compare spacy tagger with sparv's tags\n",
    "class WhitespaceTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(\" \")\n",
    "        return Doc(self.vocab, words=words)\n",
    "\n",
    "nlp = spacy.load(os.path.join(\"..\", \"sv_model_upos\", \"sv_model0\", \"sv_model_upos0-0.0.0\"))\n",
    "#nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at distribution of sentence lengths across languages/times\n",
    "\"\"\"\n",
    "    Process: pass in the function that will process individual sentences\n",
    "    Returns whatever process returns\n",
    "\"\"\"\n",
    "def traverseSwed(filepath, process, relative=False):\n",
    "    parser = ET.iterparse(file_path, events=('start','end'))\n",
    "    parser = iter(parser)\n",
    "    event, root = next(parser)\n",
    "\n",
    "    words = \"\"\n",
    "    pos = []\n",
    "    sum_overlaps= 0 \n",
    "    num_para = 0\n",
    "    i = 0\n",
    "    ag = []\n",
    "    for event, elem in parser:\n",
    "        if elem.tag == \"w\":\n",
    "            if event == \"end\":\n",
    "                #print(elem.attrib['pos'])\n",
    "                words +=  elem.text + \" \"\n",
    "                pos.append(elem.attrib['pos'])\n",
    "                elem.clear()\n",
    "            root.clear()\n",
    "        if elem.tag == \"sentence\" and event==\"end\":\n",
    "            words = words.strip()\n",
    "            if len(words) == 0: continue\n",
    "            doc = nlp(words)\n",
    "            tok = list(doc.sents)[0].root\n",
    "            if process != sentenceLength and relative:\n",
    "                ag += [process(tok)/len(doc)]\n",
    "            else: \n",
    "                ag += [process(tok)]\n",
    "            \n",
    "            words = \"\"\n",
    "            pos = []\n",
    "            i += 1\n",
    "        if i == 10000: break\n",
    "    return ag\n",
    "            \n",
    "def traverseFin(filepath, process, relative=False):\n",
    "    ag = []\n",
    "    num_sentences = 0\n",
    "    with open(file_path) as reader:\n",
    "        words = []\n",
    "        pos = []\n",
    "        for line in reader:\n",
    "            if num_sentences == 10000: return sum_overlaps / num_sentences\n",
    "            # Start of sentence\n",
    "            if line[:9] == \"<sentence\": \n",
    "                #print(\"start sentence\")\n",
    "                words = []\n",
    "                pos_spacy=pos_sparv=pos=[]\n",
    "            # End of sentence\n",
    "            if line[:11] == \"</sentence>\": \n",
    "                if(len(words)) == 0: continue\n",
    "                sent = \" \".join(words)\n",
    "                # print(words)\n",
    "                doc = nlp(sent)\n",
    "                tok = list(doc.sents)[0].root\n",
    "                if process != sentenceLength and relative: \n",
    "                    ag += [process(tok)/len(doc)]\n",
    "                else: \n",
    "                    ag += [process(tok)]\n",
    "                \n",
    "                num_sentences += 1\n",
    "            # Split up by sentence\n",
    "            # If at a text line and not just tags\n",
    "            if line[0] != \"<\": \n",
    "                wordInfo = line.split()\n",
    "                #print(wordInfo)\n",
    "                words += [wordInfo[0]]\n",
    "                pos += [wordInfo[1]]\n",
    "            if num_sentences == 10000: break\n",
    "    \n",
    "    return ag\n",
    "\n",
    "def sentenceLength(tok):\n",
    "    # print(doc, len(doc))\n",
    "    return len(list(tok.children)) + 1\n",
    "\n",
    "def treeDepth(tok):\n",
    "    if len(list(tok.children)) == 0: return 0 \n",
    "    \n",
    "    depths = np.array([treeDepth(c) for c in tok.children]).flatten()\n",
    "    return max(depths) + 1\n",
    "\n",
    "def numLeafs(tok): \n",
    "    if len(list(tok.children)) == 0: return 1\n",
    "    \n",
    "    leafs = np.array([numLeafs(c) for c in tok.children]).flatten()\n",
    "    return sum(leafs)\n",
    "\n",
    "\"\"\"\n",
    "    For a sentence with the root token tok, return the breakdown on the dependency relations\n",
    "\"\"\"\n",
    "def numComponents(tok): \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
