{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tagger Testing\n",
    "This notebook is the result of initial experimentation with spacy's Swedish POS tagger. I wanted to get a sense for the difference between the taggers, and how they each perform across time. The hypothesis is that the sparv tagset will perform worse on older documents compared to spacy's tagset. They will both however likely struggle on older texts due to the poor level of OCR. There's no baseline we can use as a ground truth, so the sanity check is to see if the POS tags overlap less the further back you go in time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "# Tokenize only on whitespace to compare spacy tagger with sparv's tags\n",
    "class WhitespaceTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __call__(self, text):\n",
    "        words = text.split(\" \")\n",
    "        return Doc(self.vocab, words=words)\n",
    "\n",
    "nlp = spacy.load(os.path.join(\"..\", \"sv_model_upos\", \"sv_model0\", \"sv_model_upos0-0.0.0\"))\n",
    "nlp.tokenizer = WhitespaceTokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`msd_to_upos` dictionary converts sparv's POS tags to spacy POS tagset.  \n",
    "MSD tagstet found [here](https://spraakbanken.gu.se/korp/markup/msdtags.html).  \n",
    "UPOS tagset found [here](https://universaldependencies.org/u/pos/) as universal POS tags.  \n",
    "TODO: Consider using the [XPOS tagger](https://universaldependencies.org/sv/index.html) instead so that more specific to Swedish texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "msd_to_upos = {\n",
    "    \"PM\": \"PROPN\",\n",
    "    \"VB\": \"VERB\",\n",
    "    \"MID\": \"PUNCT\",\n",
    "    \"MAD\": \"PUNCT\",\n",
    "    \"PP\": \"ADP\", \n",
    "    \"PC\": \"ADJ\", # Participle = adjective\n",
    "    \"KN\": \"CCONJ\",\n",
    "    \"JJ\": \"AJD\", \n",
    "    \"NN\": \"NOUN\", \n",
    "    \"RG\": \"NUM\", # Cardinal number = number\n",
    "    \"RO\": \"NUM\", # Ordinal Number = number\n",
    "    \"DT\": \"DET\", \n",
    "    \"PN\": \"PRON\", # Pronoun = pronoun\n",
    "    \"AB\": \"ADV\", # Adverb = adverb\n",
    "    \"PAD\": \"PUNCT\", # Pairwise delimiter = punctuation\n",
    "    \"AN\": \"PUNCT\", # Abbreviation = punctuation\n",
    "    \"SN\": \"SCONJ\", # Subjunction = subordinating conjunction\n",
    "    \"UO\": \"X\", # Foreign word = other\n",
    "    \"IE\": \"PART\", # Infinitive marker = particle\n",
    "    \"PS\": \"PART\", # Possessive = particle\n",
    "    \"HA\": \"ADV\", # Relative adverb = adverb\n",
    "    \"HP\": \"PRON\", # Relative pronoun = pronoun\n",
    "    \"HS\": \"PART\", # Relative possessive = particle\n",
    "    \"IN\": \"INTJ\", # Interjection\n",
    "    \"PL\": \"PART\", # Particle\n",
    "    \"NL\" : \"NUM\", # Spelled out words = NUM\n",
    "    \"HD\": \"DET\", # Relative determiner = determiner\n",
    "}\n",
    "\n",
    "msd_to_xpos= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPos(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return [token.tag_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPos(sentence, origTags):\n",
    "    doc = nlp(sentence)\n",
    "    width = 15\n",
    "    for idx, token in enumerate(doc):\n",
    "        print(f\"{token.text: <{width}} {token.tag_: <{width}} {msd_to_upos[origTags[idx]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Simply compare POS tags\n",
    "- Documents published later in time seem to match POS tags pretty closely with what spacy produces\n",
    "- Spacy tokenizer by default splits on punctuation and sybmols, while sparv seems to group together more. \n",
    "- Replaced Spacy's defautl tokenizer with a whitespace tokenizer for better comparisons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 Skandia\n",
      "----\n",
      "Skandia         NOUN            PROPN\n",
      "None\n",
      "----\n",
      "\n",
      "6 6 meddela , - genom undertecknad :\n",
      "----\n",
      "meddela         VERB            VERB\n",
      ",               PUNCT           PUNCT\n",
      "-               PUNCT           PUNCT\n",
      "genom           ADP             ADP\n",
      "undertecknad    NOUN            ADJ\n",
      ":               PUNCT           PUNCT\n",
      "None\n",
      "----\n",
      "\n",
      "40 40 Lillorsäkriugar , Brandförsäkringar , Lifräntor , Kapitalförsäkringar , Utstyrselförsäkringar och försäkring mot explosion af ånga eller lysgas . Prospekter , ansökningsformulärer och nödige upplysningar lemnäs i Karlskrona af undertecknad och i Ronneby af Herr August Carlsson . Carl Frick ,\n",
      "----\n",
      "Lillorsäkriugar NOUN            NOUN\n",
      ",               PUNCT           PUNCT\n",
      "Brandförsäkringar NOUN            NOUN\n",
      ",               PUNCT           PUNCT\n",
      "Lifräntor       NOUN            NOUN\n",
      ",               PUNCT           PUNCT\n",
      "Kapitalförsäkringar NOUN            NOUN\n",
      ",               PUNCT           PUNCT\n",
      "Utstyrselförsäkringar NOUN            NOUN\n",
      "och             CCONJ           CCONJ\n",
      "försäkring      NOUN            NOUN\n",
      "mot             ADP             ADP\n",
      "explosion       NOUN            NOUN\n",
      "af              ADJ             ADP\n",
      "ånga            NOUN            NOUN\n",
      "eller           CCONJ           CCONJ\n",
      "lysgas          NOUN            NOUN\n",
      ".               PUNCT           PUNCT\n",
      "Prospekter      NOUN            NOUN\n",
      ",               PUNCT           PUNCT\n",
      "ansökningsformulärer NOUN            NOUN\n",
      "och             CCONJ           CCONJ\n",
      "nödige          ADJ             AJD\n",
      "upplysningar    NOUN            NOUN\n",
      "lemnäs          NOUN            PROPN\n",
      "i               ADP             ADP\n",
      "Karlskrona      PROPN           PROPN\n",
      "af              NUM             ADP\n",
      "undertecknad    ADJ             ADJ\n",
      "och             CCONJ           CCONJ\n",
      "i               ADP             ADP\n",
      "Ronneby         PROPN           PROPN\n",
      "af              PROPN           ADP\n",
      "Herr            NOUN            NOUN\n",
      "August          PROPN           PROPN\n",
      "Carlsson        PROPN           PROPN\n",
      ".               PUNCT           PUNCT\n",
      "Carl            PROPN           PROPN\n",
      "Frick           PROPN           PROPN\n",
      ",               PUNCT           PUNCT\n",
      "None\n",
      "----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(\"/share/magpie/datasets/Swedish\", \"kubhist2\",\"kubhist2-carlscronaswekoblad-1870.xml\")\n",
    "parser = ET.iterparse(file_path, events=('start','end'))\n",
    "parser = iter(parser)\n",
    "event, root = next(parser)\n",
    "\n",
    "words = \"\"\n",
    "pos = []\n",
    "i = 0\n",
    "for event, elem in parser:\n",
    "    if elem.tag == \"w\":\n",
    "        if event == \"end\":\n",
    "            #print(elem.attrib['pos'])\n",
    "            words +=  elem.text + \" \"\n",
    "            pos.append(elem.attrib['pos'])\n",
    "            elem.clear()\n",
    "        root.clear()\n",
    "    if elem.tag == \"paragraph\" and event==\"end\":\n",
    "        words = words.strip()\n",
    "        print(len(words.split()), len(pos), words)\n",
    "        print(\"----\")\n",
    "        print(showPos(words, pos))\n",
    "        print(\"----\\n\")\n",
    "        words = \"\"\n",
    "        pos =[]\n",
    "        i +=1\n",
    "    if i == 3: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Get a feel for how taggers compare across time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Use the first 10,000 paragraphs to get a sense of how the spacy and sparv POS tags compare. \n",
    "    TODO: Consider stopping condition of when average seems to settle, doesn't deviate more than some epsilon each checkpoint. \n",
    "\"\"\"\n",
    "def getPosOverlap(file_path):\n",
    "    parser = ET.iterparse(file_path, events=('start','end'))\n",
    "    parser = iter(parser)\n",
    "    event, root = next(parser)\n",
    "\n",
    "    words = \"\"\n",
    "    pos = []\n",
    "    sum_overlaps= 0 \n",
    "    num_para = 0\n",
    "    i = 0\n",
    "    for event, elem in parser:\n",
    "        if elem.tag == \"w\":\n",
    "            if event == \"end\":\n",
    "                #print(elem.attrib['pos'])\n",
    "                words +=  elem.text + \" \"\n",
    "                pos.append(elem.attrib['pos'])\n",
    "                elem.clear()\n",
    "            root.clear()\n",
    "        if elem.tag == \"paragraph\" and event==\"end\":\n",
    "            words = words.strip()\n",
    "            #print(\"ORIG:\", [f'{words.split()[i]}/{pos[i]}' for i in range(len(words.split()))] )\n",
    "            try:\n",
    "                pos_sparv =  np.array([msd_to_upos[p] for p in pos])\n",
    "            except KeyError as k: \n",
    "                print(k,i)\n",
    "                print(\"ORIG:\", [f'{words.split()[i]}/{pos[i]}' for i in range(len(words.split()))] )\n",
    "                words = \"\"\n",
    "                pos = []\n",
    "                i += 1\n",
    "                continue\n",
    "            doc = nlp(words)\n",
    "            pos_spacy = [token.tag_ for token in doc]\n",
    "            #seq_sparv = [f'{words.split()[i]}/{pos_sparv[i]}' for i in range(len(words.split()))]\n",
    "            #seq_spacy = [f'{token.text}/{token.tag_}' for token in doc]\n",
    "            #print(\"\\nSPARV:\", [f'{words.split()[i]}/{pos_sparv[i]}' for i in range(len(words.split()))])\n",
    "            #print(\"SPACY:\", [f'{token.text}/{token.tag_}' for token in doc])\n",
    "            #print(len(words.split()), len([token.text for token in doc]), len(pos_spacy))\n",
    "            sum_overlaps += sum(pos_sparv== pos_spacy)/len(pos_sparv)\n",
    "\n",
    "            words = \"\"\n",
    "            pos =[]\n",
    "            i +=1\n",
    "        if i % 1000 == 0 and i != 0: \n",
    "            num_para = i\n",
    "            # print(sum_overlaps/i)\n",
    "        if i == 10000: break\n",
    "    num_para = i\n",
    "    return sum_overlaps/num_para"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare taggers across time in Swedish\n",
    "The overlap between POS tags seems to decrease as you go back in time, confirming the sanity check. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7027425644779666"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.join(\"/share/magpie/datasets/Swedish\", \"kubhist2\",\"kubhist2-carlscronaswekoblad-1870.xml\")\n",
    "getPosOverlap(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.666436125500888"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = os.path.join(\"/share/magpie/datasets/Swedish\", \"kubhist2\",\"kubhist2-carlscronaswekoblad-1760.xml\")\n",
    "getPosOverlap(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare taggers across languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"</sentence>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPosOverlapFin(file_path):\n",
    "    num_sentences = 0\n",
    "    sum_overlaps= 0 \n",
    "    with open(file_path) as reader:\n",
    "        words = []\n",
    "        pos = []\n",
    "        for line in reader:\n",
    "            if num_sentences == 10000: return sum_overlaps / num_sentences\n",
    "            # Start of sentence\n",
    "            if line[:9] == \"<sentence\": \n",
    "                #print(\"start sentence\")\n",
    "                words = []\n",
    "                pos_spacy=pos_sparv=pos=[]\n",
    "            # End of sentence\n",
    "            if line[:11] == \"</sentence>\": \n",
    "                #print(\"end sentence\")\n",
    "                sent = \" \".join(words)\n",
    "                doc = nlp(sent)\n",
    "                pos_spacy = np.array([token.tag_ for token in doc])\n",
    "                pos_sparv = np.array([msd_to_upos[p] for p in pos])\n",
    "                # print(\"\\nSPARV:\", [f'{words[i]}/{pos_sparv[i]}' for i in range(len(words))])\n",
    "                # print(\"SPACY:\", [f'{token.text}/{token.tag_}' for token in doc])\n",
    "                # print(len(pos_spacy), len(pos_sparv))\n",
    "                sum_overlaps += (sum(pos_sparv== pos_spacy)/len(pos_sparv))\n",
    "                # print((sum(pos_sparv== pos_spacy)/len(pos_sparv)))\n",
    "                num_sentences += 1\n",
    "            # Split up by sentence\n",
    "            # If at a text line and not just tags\n",
    "            if line[0] != \"<\": \n",
    "                wordInfo = line.split()\n",
    "                #print(wordInfo)\n",
    "                words += [wordInfo[0]]\n",
    "                pos += [wordInfo[1]]\n",
    "    return sum_overlaps / num_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finnish, 1870: \t 0.6972330339073696\n",
      "Finnish, 1771: \t 0.6685390123764982\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(\"/share/magpie/datasets/Swedish\", \"klk-sv-1771-1879-vrt\", \"klk-sv-1870.vrt\")\n",
    "print(\"Finnish, 1870: \\t\", getPosOverlapFin(file_path))\n",
    "\n",
    "file_path = os.path.join(\"/share/magpie/datasets/Swedish\", \"klk-sv-1771-1879-vrt\", \"klk-sv-1771.vrt\")\n",
    "print(\"Finnish, 1771: \\t\", getPosOverlapFin(file_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
